{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5acb991d",
   "metadata": {},
   "source": [
    "# TEST MODEL 1 TANGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788e1938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.ClassificationModule import Classifier\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Inisialisasi komponen utama\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = HandDetector(maxHands=1, detectionCon=0.8, minTrackCon=0.7)\n",
    "classifier = Classifier(\"model/keras_model.h5\", \"model/labels.txt\")\n",
    "\n",
    "# Pengaturan untuk cropping dan resizing gambar\n",
    "offset = 30\n",
    "imgSize = 300\n",
    "labels = [\"C\", \"L\", \"V\", \"I\"]\n",
    "    \n",
    "# --- Variabel Baru untuk Fungsi Penyusunan Gestur ---\n",
    "sentence = \"\"               # Untuk menyimpan kalimat yang terbentuk\n",
    "last_capture_time = 0       # Waktu terakhir gestur diambil\n",
    "capture_cooldown = 2        # Jeda waktu 2 detik\n",
    "capture_animation_counter = 0 # Timer untuk animasi \"screenshot\"\n",
    "\n",
    "def draw_sentence_box(img, text):\n",
    "    \"\"\"Fungsi untuk menggambar kotak teks dan kalimat di bagian bawah gambar.\"\"\"\n",
    "    # Ambil dimensi gambar\n",
    "    h, w, _ = img.shape\n",
    "    \n",
    "    # Buat overlay untuk background semi-transparan\n",
    "    overlay = img.copy()\n",
    "    \n",
    "    # Tentukan posisi dan ukuran kotak\n",
    "    box_start_point = (50, h - 110)\n",
    "    box_end_point = (w - 50, h - 50)\n",
    "    \n",
    "    # Gambar kotak background\n",
    "    cv2.rectangle(overlay, box_start_point, box_end_point, (0, 0, 0), -1)\n",
    "    \n",
    "    # Gabungkan overlay dengan gambar asli untuk efek transparan\n",
    "    alpha = 0.5 # Tingkat transparansi\n",
    "    img = cv2.addWeighted(overlay, alpha, img, 1 - alpha, 0)\n",
    "    \n",
    "    # Tambahkan teks kalimat di atas kotak\n",
    "    # Menambahkan kursor sederhana '|' jika kalimat tidak kosong\n",
    "    display_text = text + \"_\" if text else \"_\"\n",
    "    \n",
    "    cv2.putText(img, display_text, (65, h - 70), cv2.FONT_HERSHEY_COMPLEX, 1.5, (255, 255, 255), 2)\n",
    "    return img\n",
    "\n",
    "# Loop utama program\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    img = cv2.flip(img, 1)\n",
    "    imgOut = img.copy()\n",
    "    hands, img = detector.findHands(img, flipType=False)\n",
    "\n",
    "    if hands:\n",
    "        hand = hands[0]\n",
    "        x, y, w, h = hand['bbox']\n",
    "\n",
    "        # Pastikan area crop tidak keluar dari frame\n",
    "        x_min = max(0, x - offset)\n",
    "        y_min = max(0, y - offset)\n",
    "        x_max = min(img.shape[1], x + w + offset)\n",
    "        y_max = min(img.shape[0], y + h + offset)\n",
    "\n",
    "        imgCrop = img[y_min:y_max, x_min:x_max]\n",
    "        \n",
    "        # Lanjutkan hanya jika imgCrop tidak kosong\n",
    "        if imgCrop.size > 0:\n",
    "            imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255\n",
    "            \n",
    "            # Logika resizing dengan menjaga aspect ratio\n",
    "            aspectRatio = h / w\n",
    "            if aspectRatio > 1:\n",
    "                k = imgSize / h\n",
    "                wCal = math.ceil(k * w)\n",
    "                imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "                wGap = math.ceil((imgSize - wCal) / 2)\n",
    "                imgWhite[:, wGap:wCal + wGap] = imgResize\n",
    "            else:\n",
    "                # Perbaikan bug: seharusnya imgSize / w\n",
    "                k = imgSize / w \n",
    "                hCal = math.ceil(k * h)\n",
    "                imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "                hGap = math.ceil((imgSize - hCal) / 2)\n",
    "                imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "\n",
    "            # Dapatkan prediksi\n",
    "            prediction, index = classifier.getPrediction(imgWhite)\n",
    "\n",
    "            # --- Logika Baru: Ambil Gestur Setiap 2 Detik ---\n",
    "            current_time = time.time()\n",
    "            if current_time - last_capture_time > capture_cooldown:\n",
    "                # Tambahkan label ke kalimat\n",
    "                sentence += labels[index]\n",
    "                # Reset timer\n",
    "                last_capture_time = current_time\n",
    "                # Mulai animasi screenshot\n",
    "                capture_animation_counter = 5 # Animasi akan berlangsung selama 5 frame\n",
    "            \n",
    "            # Tampilkan label prediksi di dekat tangan\n",
    "            cv2.rectangle(imgOut, (x - offset, y - offset - 40), (x - offset + 80, y - offset), (255, 0, 255), -1)\n",
    "            cv2.putText(imgOut, labels[index], (x - offset + 5, y - offset - 10), cv2.FONT_HERSHEY_COMPLEX, 1.2, (255, 255, 255), 2)\n",
    "            # cv2.imshow(\"White Image\", imgWhite) # Opsional, bisa di-disable\n",
    "\n",
    "    # --- Logika Baru: Animasi \"Screenshot\" ---\n",
    "    if capture_animation_counter > 0:\n",
    "        h_out, w_out, _ = imgOut.shape\n",
    "        overlay = imgOut.copy()\n",
    "        # Buat layar menjadi putih semi-transparan\n",
    "        cv2.rectangle(overlay, (0, 0), (w_out, h_out), (255, 255, 255), -1)\n",
    "        # Terapkan efeknya\n",
    "        imgOut = cv2.addWeighted(overlay, 0.3, imgOut, 0.7, 0)\n",
    "        # Kurangi counter animasi\n",
    "        capture_animation_counter -= 1\n",
    "\n",
    "    # --- Logika Baru: Tampilkan Kotak Kalimat ---\n",
    "    imgOut = draw_sentence_box(imgOut, sentence)\n",
    "\n",
    "    cv2.imshow(\"Image\", imgOut)\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    # Tambahkan fitur untuk menghapus huruf terakhir dengan tombol backspace\n",
    "    if key == 8 and len(sentence) > 0: # 8 adalah ASCII untuk backspace\n",
    "        sentence = sentence[:-1]\n",
    "    # Tambahkan fitur untuk menghapus semua kalimat dengan tombol 'c'\n",
    "    elif key == ord('c'):\n",
    "        sentence = \"\"\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133cb4ad",
   "metadata": {},
   "source": [
    "# Test Model 2 Tangan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0edbc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1757746845.225721  185303 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1757746845.228060  187831 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: AMD Radeon Graphics (radeonsi, renoir, ACO, DRM 3.61, 6.14.0-29-generic)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 1s 727ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.ClassificationModule import Classifier\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "# --- SETUP ---\n",
    "# Initialize video capture, hand detector, and classifier\n",
    "cap = cv2.VideoCapture(0)\n",
    "# Set maxHands to 2 to detect both hands\n",
    "detector = HandDetector(maxHands=2, detectionCon=0.8, minTrackCon=0.7) \n",
    "# IMPORTANT: Load your trained model for two-handed gestures\n",
    "classifier = Classifier(\"model/merge/keras_model.h5\", \"model/merge/labels.txt\")\n",
    "\n",
    "# --- PARAMETERS ---\n",
    "# Offset for the bounding box\n",
    "offset = 30\n",
    "# Size for the square image fed to the model\n",
    "imgSize = 300\n",
    "# IMPORTANT: Update this list with the labels your model can predict\n",
    "labels = [\"A\", \"B\", \"C\",\"I\", \"L\", \"V\"]\n",
    "\n",
    "# --- SENTENCE BUILDING LOGIC ---\n",
    "sentence = \"\"               # Stores the final sentence\n",
    "last_capture_time = 0       # Timer to control capture rate\n",
    "capture_cooldown = 2        # Cooldown in seconds between captures\n",
    "\n",
    "def draw_sentence_box(img, text):\n",
    "    \"\"\"\n",
    "    Draws a semi-transparent box at the bottom of the screen to display the sentence.\n",
    "    \"\"\"\n",
    "    h, w, _ = img.shape\n",
    "    overlay = img.copy()\n",
    "    box_start = (50, h - 110)\n",
    "    box_end = (w - 50, h - 50)\n",
    "    \n",
    "    cv2.rectangle(overlay, box_start, box_end, (0, 0, 0), -1)\n",
    "    \n",
    "    alpha = 0.6  # Transparency factor\n",
    "    img = cv2.addWeighted(overlay, alpha, img, 1 - alpha, 0)\n",
    "    \n",
    "    # Add a blinking cursor effect\n",
    "    display_text = text + \"_\"\n",
    "    \n",
    "    cv2.putText(img, display_text, (65, h - 70), \n",
    "                cv2.FONT_HERSHEY_COMPLEX, 1.5, (255, 255, 255), 2)\n",
    "    return img\n",
    "\n",
    "# --- MAIN LOOP ---\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    img = cv2.flip(img, 1)\n",
    "    imgOut = img.copy()\n",
    "    # Find hands but don't draw on the original `img`\n",
    "    hands, img = detector.findHands(img, flipType=False) \n",
    "\n",
    "    # --- CORE LOGIC: DETECT AND PROCESS 2 HANDS ---\n",
    "    # This block runs only when exactly two hands are detected\n",
    "    if len(hands) == 2:\n",
    "        # Get bounding box for the first hand\n",
    "        x1, y1, w1, h1 = hands[0]['bbox']\n",
    "        # Get bounding box for the second hand\n",
    "        x2, y2, w2, h2 = hands[1]['bbox']\n",
    "\n",
    "        # Create a combined bounding box that covers both hands\n",
    "        x_min = max(0, min(x1, x2) - offset)\n",
    "        y_min = max(0, min(y1, y2) - offset)\n",
    "        x_max = min(img.shape[1], max(x1 + w1, x2 + w2) + offset)\n",
    "        y_max = min(img.shape[0], max(y1 + h1, y2 + h2) + offset)\n",
    "\n",
    "        # Get the width and height of the combined box\n",
    "        w_combined = x_max - x_min\n",
    "        h_combined = y_max - y_min\n",
    "\n",
    "        # Crop the image to the combined bounding box\n",
    "        imgCrop = img[y_min:y_max, x_min:x_max]\n",
    "        \n",
    "        # Proceed only if the cropped image is valid\n",
    "        if imgCrop.size > 0:\n",
    "            imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255\n",
    "            \n",
    "            # --- RESIZE AND CENTER THE GESTURE IMAGE ---\n",
    "            aspectRatio = h_combined / w_combined\n",
    "            if aspectRatio > 1: # If height is greater than width\n",
    "                k = imgSize / h_combined\n",
    "                wCal = math.ceil(k * w_combined)\n",
    "                imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "                wGap = math.ceil((imgSize - wCal) / 2)\n",
    "                imgWhite[:, wGap:wCal + wGap] = imgResize\n",
    "            else: # If width is greater than or equal to height\n",
    "                k = imgSize / w_combined\n",
    "                hCal = math.ceil(k * h_combined)\n",
    "                imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "                hGap = math.ceil((imgSize - hCal) / 2)\n",
    "                imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "\n",
    "            # --- PREDICTION AND SENTENCE LOGIC ---\n",
    "            # Get prediction from the classifier\n",
    "            prediction, index = classifier.getPrediction(imgWhite)\n",
    "\n",
    "            # Check if cooldown has passed\n",
    "            current_time = time.time()\n",
    "            if current_time - last_capture_time > capture_cooldown:\n",
    "                # Add a space if the sentence is not empty\n",
    "                if sentence:\n",
    "                    sentence += \" \"\n",
    "                # Append the predicted label to the sentence\n",
    "                sentence += labels[index]\n",
    "                # Reset the timer\n",
    "                last_capture_time = current_time\n",
    "            \n",
    "            # --- DISPLAY PREDICTION ON SCREEN ---\n",
    "            # Draw a box and the predicted label above the combined gesture\n",
    "            cv2.rectangle(imgOut, (x_min, y_min - 40), \n",
    "                          (x_min + 150, y_min), (255, 0, 255), -1)\n",
    "            cv2.putText(imgOut, labels[index], (x_min + 5, y_min - 10), \n",
    "                        cv2.FONT_HERSHEY_COMPLEX, 1.2, (255, 255, 255), 2)\n",
    "            # Draw the combined bounding box\n",
    "            cv2.rectangle(imgOut, (x_min, y_min), (x_max, y_max), (255, 0, 255), 2)\n",
    "            cv2.imshow(\"White Image\", imgWhite) # Optional, can be disabled\n",
    "\n",
    "    # --- UI AND CONTROLS ---\n",
    "    # Display the sentence box on the output image\n",
    "    imgOut = draw_sentence_box(imgOut, sentence)\n",
    "    cv2.imshow(\"BISINDO Gesture Translator\", imgOut)\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    # Quit with 'q'\n",
    "    if key & 0xFF == ord('q'):\n",
    "        break\n",
    "    # Delete last word with Backspace\n",
    "    if key == 8 and sentence: # 8 is ASCII for backspace\n",
    "        words = sentence.split(' ')\n",
    "        sentence = ' '.join(words[:-1])\n",
    "    # Clear entire sentence with 'c'\n",
    "    elif key == ord('c'):\n",
    "        sentence = \"\"\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2478ade",
   "metadata": {},
   "source": [
    "# FINAL TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fa230e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1757746956.330966  185303 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1757746956.333012  191690 gl_context.cc:344] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: AMD Radeon Graphics (radeonsi, renoir, ACO, DRM 3.61, 6.14.0-29-generic)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "1/1 [==============================] - 1s 707ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "from cvzone.ClassificationModule import Classifier\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "\n",
    "# --- SETUP ---\n",
    "# Inisialisasi Video Capture, Hand Detector, dan Classifier\n",
    "cap = cv2.VideoCapture(0)\n",
    "# PENTING: Set maxHands ke 2 agar bisa mendeteksi satu atau dua tangan\n",
    "detector = HandDetector(maxHands=2, detectionCon=0.8, minTrackCon=0.7) \n",
    "# PENTING: Ganti dengan path model dan label BISINDO baru Anda\n",
    "classifier = Classifier(\"model/merge/keras_model.h5\", \"model/merge/labels.txt\")\n",
    "\n",
    "# --- PARAMETER ---\n",
    "offset = 30         # Offset untuk bounding box agar lebih lega\n",
    "imgSize = 300       # Ukuran gambar input untuk model\n",
    "\n",
    "# PENTING: Update list ini sesuai dengan isi file labels.txt Anda\n",
    "# Pastikan urutannya sama persis dengan yang ada di file label.\n",
    "labels = [\"A\", \"B\", \"C\",\"I\", \"L\", \"V\"] # Contoh label, sesuaikan dengan milik Anda\n",
    "\n",
    "# --- LOGIKA PENYUSUNAN KALIMAT ---\n",
    "sentence = \"\"               # Menyimpan kalimat yang terbentuk\n",
    "last_capture_time = 0       # Timer untuk mengontrol     kecepatan penangkapan gestur\n",
    "capture_cooldown = 2        # Jeda 2 detik antar penangkapan gestur\n",
    "\n",
    "def draw_sentence_box(img, text):\n",
    "    \"\"\"\n",
    "    Menggambar kotak semi-transparan di bagian bawah layar untuk menampilkan kalimat.\n",
    "    \"\"\"\n",
    "    h, w, _ = img.shape\n",
    "    overlay = img.copy()\n",
    "    box_start = (50, h - 110)\n",
    "    box_end = (w - 50, h - 50)\n",
    "    \n",
    "    cv2.rectangle(overlay, box_start, box_end, (50, 50, 50), -1) # Kotak abu-abu gelap\n",
    "    \n",
    "    alpha = 0.6  # Faktor transparansi\n",
    "    img = cv2.addWeighted(overlay, alpha, img, 1 - alpha, 0)\n",
    "    \n",
    "    # Menambahkan efek kursor berkedip sederhana\n",
    "    display_text = text + \"_\"\n",
    "    \n",
    "    cv2.putText(img, display_text, (65, h - 70), \n",
    "                cv2.FONT_HERSHEY_COMPLEX, 1.5, (255, 255, 255), 2)\n",
    "    return img\n",
    "\n",
    "# --- LOOP UTAMA ---\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        print(\"Gagal mengambil frame dari kamera.\")\n",
    "        break\n",
    "\n",
    "    img = cv2.flip(img, 1) # Flip gambar agar seperti cermin\n",
    "    imgOut = img.copy()\n",
    "    \n",
    "    # Temukan tangan dalam gambar\n",
    "    hands, img = detector.findHands(img, flipType=False) \n",
    "\n",
    "    imgCrop = None\n",
    "    bbox_display = None\n",
    "    \n",
    "    # --- LOGIKA DINAMIS: CEK JUMLAH TANGAN ---\n",
    "    if hands:\n",
    "        # --- KASUS 1: HANYA SATU TANGAN TERDETEKSI ---\n",
    "        if len(hands) == 1:\n",
    "            hand = hands[0]\n",
    "            x, y, w, h = hand['bbox']\n",
    "            bbox_display = (x, y, w, h)\n",
    "            \n",
    "            # Crop gambar tangan dengan offset\n",
    "            x_min = max(0, x - offset)\n",
    "            y_min = max(0, y - offset)\n",
    "            x_max = min(img.shape[1], x + w + offset)\n",
    "            y_max = min(img.shape[0], y + h + offset)\n",
    "            imgCrop = img[y_min:y_max, x_min:x_max]\n",
    "\n",
    "        # --- KASUS 2: DUA TANGAN TERDETEKSI ---\n",
    "        elif len(hands) == 2:\n",
    "            # Dapatkan bounding box untuk masing-masing tangan\n",
    "            x1, y1, w1, h1 = hands[0]['bbox']\n",
    "            x2, y2, w2, h2 = hands[1]['bbox']\n",
    "\n",
    "            # Buat bounding box gabungan yang mencakup kedua tangan\n",
    "            x_min = max(0, min(x1, x2) - offset)\n",
    "            y_min = max(0, min(y1, y2) - offset)\n",
    "            x_max = min(img.shape[1], max(x1 + w1, x2 + w2) + offset)\n",
    "            y_max = min(img.shape[0], max(y1 + h1, y2 + h2) + offset)\n",
    "            \n",
    "            bbox_display = (x_min, y_min, x_max - x_min, y_max - y_min)\n",
    "            \n",
    "            # Crop gambar sesuai bounding box gabungan\n",
    "            imgCrop = img[y_min:y_max, x_min:x_max]\n",
    "            \n",
    "    # --- PROSES GAMBAR DAN PREDIKSI (JIKA TANGAN TERDETEKSI) ---\n",
    "    if imgCrop is not None and imgCrop.size > 0:\n",
    "        imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255\n",
    "        \n",
    "        # --- RESIZE DAN PUSATKAN GAMBAR GESTUR ---\n",
    "        h_crop, w_crop, _ = imgCrop.shape\n",
    "        aspectRatio = h_crop / w_crop\n",
    "        \n",
    "        if aspectRatio > 1: # Jika tinggi lebih besar dari lebar\n",
    "            k = imgSize / h_crop\n",
    "            wCal = math.ceil(k * w_crop)\n",
    "            imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "            wGap = math.ceil((imgSize - wCal) / 2)\n",
    "            imgWhite[:, wGap:wCal + wGap] = imgResize\n",
    "        else: # Jika lebar lebih besar atau sama dengan tinggi\n",
    "            k = imgSize / w_crop\n",
    "            hCal = math.ceil(k * h_crop)\n",
    "            imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "            hGap = math.ceil((imgSize - hCal) / 2)\n",
    "            imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "\n",
    "        # --- PREDIKSI DAN LOGIKA KALIMAT ---\n",
    "        prediction, index = classifier.getPrediction(imgWhite, draw=False)\n",
    "\n",
    "        current_time = time.time()\n",
    "        if current_time - last_capture_time > capture_cooldown:\n",
    "            if sentence and labels[index] != sentence.split(\" \")[-1]:\n",
    "                sentence += \" \"\n",
    "            if not sentence or labels[index] != sentence.split(\" \")[-1]:\n",
    "                sentence += labels[index]\n",
    "            \n",
    "            last_capture_time = current_time\n",
    "        \n",
    "        # --- TAMPILKAN PREDIKSI DI LAYAR ---\n",
    "        if bbox_display:\n",
    "            x, y, w, h = bbox_display\n",
    "            cv2.rectangle(imgOut, (x - offset, y - offset), (x + w + offset, y + h + offset), (255, 0, 255), 2)\n",
    "            cv2.rectangle(imgOut, (x - offset, y - offset - 40), (x - offset + 150, y - offset), (255, 0, 255), -1)\n",
    "            cv2.putText(imgOut, labels[index], (x - offset + 5, y - offset - 10), cv2.FONT_HERSHEY_COMPLEX, 1.2, (255, 255, 255), 2)\n",
    "            cv2.imshow(\"Gesture Crop\", imgWhite) # Aktifkan jika ingin melihat gambar input model\n",
    "\n",
    "    # --- UI DAN KONTROL ---\n",
    "    imgOut = draw_sentence_box(imgOut, sentence)\n",
    "    cv2.imshow(\"BISINDO Gesture Translator\", imgOut)\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    # Keluar dengan tombol 'q'\n",
    "    if key & 0xFF == ord('q'):\n",
    "        break\n",
    "    # Hapus kata terakhir dengan Backspace\n",
    "    if key == 8 and sentence: # 8 adalah ASCII untuk backspace\n",
    "        words = sentence.split(' ')\n",
    "        sentence = ' '.join(words[:-1])\n",
    "    # Hapus seluruh kalimat dengan 'c'\n",
    "    elif key == ord('c'):\n",
    "        sentence = \"\"\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
